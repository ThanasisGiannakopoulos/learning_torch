{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7cdd3c-c70d-4621-90ea-8ad1be806144",
   "metadata": {},
   "source": [
    "Just an attempt; but a massive fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63973009-c01b-4846-930d-e760883dcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558fed9-e2f4-4faa-8b4e-e8eed8faf470",
   "metadata": {},
   "source": [
    "The system:\n",
    "\n",
    "\\begin{align}\n",
    "\\partial_x A &= A \\left[ \\frac{1-A}{x(1-x)} + \\frac{8 \\pi x A \\rho}{(1-x)^3} \\right] \\,,\n",
    "\\\\\n",
    "\\partial_x \\alpha & = \\alpha \\left[ \\frac{A-1}{2x(1-x)} + \\frac{4 \\pi A S_A}{(1-x)^3} \\right] \\,,\n",
    "\\\\\n",
    "\\partial_x \\chi & = - \\frac{\\chi}{x(1-x)} \\left[ 1 + A - \\frac{8 \\pi x A V(\\phi)}{(1-x)} \\right] + \n",
    "    \\frac{A}{(1-x)^2} \\left[ \\frac{dV}{d\\phi} - \\left( \\frac{\\omega}{\\alpha} \\right)^2 \\phi \\right] \\,,\n",
    "\\\\\n",
    "\\partial_x \\phi & = \\frac{\\chi}{(1-x)^2}\n",
    "\\,.\n",
    "\\end{align}\n",
    "\n",
    "Boundary conditions:\n",
    "\\begin{align}\n",
    "A(x=0) = 1 \\,, \\quad \\partial_r \\alpha(x=0) = 0 \\,, \\quad \\phi(x=0) = \\phi_0 \\,, \\quad \\chi(x=0) = 0 \\,,\n",
    "\\\\\n",
    "A(x=1) = 1 \\,, \\quad \\partial_r \\alpha(x=1) = 1 \\,, \\quad \\phi(x=1) = 0 \\,, \\quad \\chi(x=1) = 0 \\,.\n",
    "\\end{align}\n",
    "\n",
    "The potential is\n",
    "$$\n",
    "V(\\phi) = \\frac{1}{2} m^2 \\phi^2 \\,, \\quad \\frac{d V}{d \\phi} = m^2 \\phi \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61301be-96fd-4ee5-a69e-e4598ff1d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/benmoseley/harmonic-oscillator-pinn/blob/main/Harmonic%20oscillator%20PINN.ipynb\n",
    "class FCN(torch.nn.Module):   \n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.Tanh\n",
    "        self.fcs = torch.nn.Sequential(*[\n",
    "                        torch.nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        self.fch = torch.nn.Sequential(*[\n",
    "                        torch.nn.Sequential(*[\n",
    "                            torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        self.fce = torch.nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec3044-8aa4-4087-9cbf-b793fa8bc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "def gradients(outputs, inputs, order = 1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1)\n",
    "    else:\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3c387-ba15-4f20-a5f2-e3c2cd2237e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random (uniform) sample points\n",
    "def random_domain_points(xmax, n):\n",
    "    #x = xmax*torch.rand((n,1), requires_grad=True) + epsilon*torch.ones((n,1), requires_grad=True)\n",
    "    x = xmax*torch.rand((n,1), requires_grad=True)\n",
    "    #if torch.is_nonzero(x)==True:\n",
    "    #    print(\"no zero\")\n",
    "    #else:\n",
    "    #    print(\"there is a zero\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac0401-6f33-46cd-a1d1-7d7b2c81c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_loss(u, x, omega, m, eps): # eps is epsilon <<1 to never have vanishing denominators \n",
    "    # torch.transpose(ux.detach(), 0, 1) remove some info and transpose, then [0] is Ax, etc\n",
    "    #A = torch.transpose(u.detach(), 0, 1)[0].view(-1,1)\n",
    "    #alpha = torch.transpose(u.detach(), 0, 1)[1].view(-1,1)\n",
    "    #chi = torch.transpose(u.detach(), 0, 1)[2].view(-1,1)\n",
    "    #phi = torch.transpose(u.detach(), 0, 1)[3].view(-1,1)\n",
    "    # output same as above\n",
    "    # from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "    A, alpha, chi, phi = map(lambda i:  u[:,[i]], range(4))\n",
    "    # take derivatives\n",
    "    Ax = gradients(A, x)\n",
    "    alphax = gradients(alpha, x)\n",
    "    chix = gradients(chi, x)\n",
    "    phix = gradients(phi, x)\n",
    "\n",
    "    # potential\n",
    "    V = 0.5*torch.pow(m,2)*torch.pow(phi,2)\n",
    "    # potential derivative wrt phi\n",
    "    dVdphi = torch.pow(m,2)*phi\n",
    "    # rho\n",
    "    rho = 0.5*(torch.pow(chi,2)/A + torch.pow((omega/alpha),2)*torch.pow(phi,2)) + V\n",
    "    # S_A\n",
    "    SA = 0.5*(torch.pow(chi,2)/A + torch.pow((omega/alpha),2)*torch.pow(phi,2)) - V\n",
    "    # eq_A is \\p_rA - rhs[A]\n",
    "    eq_A = Ax - A*( (1-A)/((x+eps)*(1-x+eps)) + 8*torch.pi*x*A*rho/torch.pow((1-x+eps),3) )\n",
    "    # eq_alpha is \\p_r alpha - rhs[alpha]\n",
    "    eq_alpha = alphax - alpha*( (A-1)/(2*(x+eps)*(1-x+eps)) + 4*torch.pi*A*SA/torch.pow((1-x+eps),3) )\n",
    "    # eq_chi  is \\p_r chi - rhs[chi]\n",
    "    eq_chi = - (chi/((x+eps)*(1-x+eps)))*(1 + A - 8*torch.pi*x*A*V/(1-x+eps)) + (A/torch.pow((1-x+eps),2))*(dVdphi - torch.pow((omega/alpha),2)*phi)\n",
    "    # eq_phi is \\p_r phi - rhs[phi]\n",
    "    eq_phi = chi/torch.pow((1-x+eps),2)\n",
    "\n",
    "    loss_dom = torch.mean(torch.pow(eq_A,2)) + torch.mean(torch.pow(eq_alpha,2)) + torch.mean(torch.pow(eq_chi,2)) + torch.mean(torch.pow(eq_phi,2))\n",
    "    return loss_dom\n",
    "\n",
    "def x0_loss(u0, x0, phi0):\n",
    "    #alphax = torch.transpose(ux0.detach(), 0, 1)[1].view(-1,1)\n",
    "    #A = torch.transpose(u0.detach(), 0, 1)[0].view(-1,1)\n",
    "    #chi = torch.transpose(u0.detach(), 0, 1)[2].view(-1,1)\n",
    "    #phi = torch.transpose(u0.detach(), 0, 1)[3].view(-1,1)\n",
    "    # from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "    A, alpha, chi, phi = map(lambda i:  u0[[i]], range(4))\n",
    "    # take derivatives\n",
    "    alphax = gradients(alpha, x0)\n",
    "    \n",
    "    loss_x0 = torch.mean(torch.pow(A-1,2)) + torch.mean(torch.pow(alphax,2)) + torch.mean(torch.pow(phi-phi0,2)) + torch.mean(torch.pow(chi,2))\n",
    "    return loss_x0\n",
    "\n",
    "def xmax_loss(umax):\n",
    "    #A = torch.transpose(umax.detach(), 0, 1)[0].view(-1,1)\n",
    "    #alpha = torch.transpose(umax.detach(), 0, 1)[1].view(-1,1)\n",
    "    #chi = torch.transpose(umax.detach(), 0, 1)[2].view(-1,1)\n",
    "    #phi = torch.transpose(umax.detach(), 0, 1)[3].view(-1,1)\n",
    "    # from https://github.com/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\n",
    "    A, alpha, chi, phi = map(lambda i:  umax[[i]], range(4))\n",
    "    \n",
    "    loss_xmax = torch.mean(torch.pow(A-1,2)) + torch.mean(torch.pow(alpha-1,2)) + torch.mean(torch.pow(phi,2)) + torch.mean(torch.pow(chi,2))\n",
    "    return loss_xmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a56d1-b9e8-4c90-876e-bae047ac31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# input 1 (x), output 4 (A, alpha, phi, chi), 32 nodes per layer, 3 hidden layers\n",
    "model = FCN(1,4,32,3)\n",
    "\n",
    "omega = torch.nn.Parameter(1*torch.ones(1, requires_grad=True))\n",
    "optimizer = torch.optim.Adam(list(model.parameters())+[omega],lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d0292-7f12-4bd0-a871-0ac718e891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2048 # number of random sampling points\n",
    "\n",
    "epochs = 1e6\n",
    "gamma1 = 10.0\n",
    "gamma2 = 10.0\n",
    "# epsilon is for the random x points, not to get the value 0\n",
    "epsilon = 1.e-4\n",
    "\n",
    "# xmax\n",
    "X = 1\n",
    "# mass\n",
    "m = 1*torch.ones(1)\n",
    "# phi(x=0)\n",
    "phi0 = 1*torch.ones(1)\n",
    "\n",
    "# lists to save things\n",
    "loss_list = []\n",
    "omegas = []\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "    # x=0\n",
    "    x0 = torch.zeros(1, requires_grad=True)\n",
    "    # xmax\n",
    "    xmax = X*torch.ones(1, requires_grad=True)\n",
    "    # time sample\n",
    "    x = random_domain_points(X, n)\n",
    "    u = model(x)\n",
    "    #print(u)\n",
    "    # Derivatives\n",
    "    #ux = torch.autograd.grad(outputs=u, \n",
    "    #                         inputs=x,\n",
    "    #                         create_graph=True,\n",
    "    #                         grad_outputs=torch.ones_like(u)\n",
    "    #                         )[0]\n",
    "    #print(ux)#? i expect 4 outputs and i get 1 (var)\n",
    "    # loss for the bulk of the domain\n",
    "    loss_dom = domain_loss(u, x, omega, m, epsilon)\n",
    "    # boundary data at x=0\n",
    "    u0 = model(x0)\n",
    "    #print(u0)\n",
    "    #ux0 = torch.autograd.grad(outputs=u0, \n",
    "    #                          inputs=x0,\n",
    "    #                          create_graph=True,\n",
    "    #                          grad_outputs=torch.ones_like(u0)\n",
    "    #                          )#[0]\n",
    "    #print(ux0)\n",
    "    loss_x0 = x0_loss(u0, x0, phi0)\n",
    "    # boundary data at x=xmax\n",
    "    umax = model(xmax)\n",
    "    loss_xmax = xmax_loss(umax)\n",
    "    # LOSS\n",
    "    loss = loss_dom + gamma1*loss_x0 + gamma2*loss_xmax\n",
    "    # save loss and omega values\n",
    "    loss_list.append(loss.detach().numpy())\n",
    "    omegas.append(omega.item())\n",
    "    # print message\n",
    "    print('epoch = ', epoch, '| loss = ', loss_list[-1], ' | omega = ', omegas[-1], '|',  end='\\r')\n",
    "    # detach() removes the \"requires_grad\" and numpy() makes it a numpy item to plot later\n",
    "    loss.backward() # This is for computing gradients using backward propagation\n",
    "    optimizer.step() # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab412465-6fd0-4ede-84f7-858473d87f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6377023-7586-4ad9-9e43-c29495533f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(omegas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98c7fd-26ed-4bdc-b1a0-a6e3b5c387b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
