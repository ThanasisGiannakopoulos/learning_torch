{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[1.],\n",
      "        [2.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.], requires_grad = True).view(-1,1)\n",
    "print('x: ',x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class to define the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Model,self).__init__()\n",
    "        self.layer01 = torch.nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        inputs = x\n",
    "        output = torch.sigmoid(self.layer01(inputs))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layer01.weight', tensor([[-0.9171]])), ('layer01.bias', tensor([-0.2038]))])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First derivative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1700],\n",
      "        [-0.0935]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "u_x  = torch.autograd.grad(outputs=u, \n",
    "                           inputs=x,\n",
    "                           create_graph=True,\n",
    "                           grad_outputs=torch.ones_like(u)\n",
    "                           )[0]\n",
    "print(u_x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u(x; \\theta)= \\sigma(ax + b)$$\n",
    "$$u_x(x; \\theta)= \\sigma'(ax + b)\\frac{\\partial}{\\partial x}(ax + b) = \\sigma(ax + b)(1 - \\sigma(ax + b))a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1700],\n",
      "        [-0.0935]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gradient check!\n",
    "a = model.state_dict()['layer01.weight']\n",
    "b = model.state_dict()['layer01.bias']\n",
    "print(torch.sigmoid(a*x + b)*(1 - torch.sigmoid(a*x + b))*a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second derivative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0793],\n",
      "        [0.0660]])\n"
     ]
    }
   ],
   "source": [
    "u_xx = torch.autograd.grad(outputs=u_x, \n",
    "                           inputs=x,\n",
    "                           grad_outputs=torch.ones_like(u_x)\n",
    "                           )[0]\n",
    "print(u_xx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u_x(x; \\theta) = \\sigma(ax + b)(1 - \\sigma(ax + b))a$$\n",
    "$$u_{xx}(x; \\theta) = \\sigma(ax + b)(1 - \\sigma(ax + b))a^2 - \\sigma(ax + b)^2(1 - \\sigma(ax + b))a^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0793],\n",
      "        [0.0660]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gradient check!\n",
    "p1 = torch.sigmoid(a*x + b)*(1 - torch.sigmoid(a*x + b))**2*a**2\n",
    "p2 = - torch.sigmoid(a*x + b)**2*(1 - torch.sigmoid(a*x + b))*a**2\n",
    "print(p1 + p2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs: If all the gradients are zero, then you get an error** (Don't know how to fix this yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layer01.weight', tensor([[-0.4659]])), ('layer01.bias', tensor([-0.1514]))])\n",
      "tensor([[-0.4659],\n",
      "        [-0.4659]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pmzib\\Dropbox\\Work\\Research\\2023\\AIM3\\Codes\\Voltage\\02_torch_grad_high_order_der_with_nn.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(lm_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m lm_xx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(outputs\u001b[39m=\u001b[39;49mlm_x, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                             inputs\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                             grad_outputs\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(lm_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                             )[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pmzib/Dropbox/Work/Research/2023/AIM3/Codes/Voltage/02_torch_grad_high_order_der_with_nn.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(lm_xx)\n",
      "File \u001b[1;32mc:\\Users\\pmzib\\Miniconda3\\envs\\torch-env-win\\lib\\site-packages\\torch\\autograd\\__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "class Linear_model(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Linear_model,self).__init__()\n",
    "        self.layer01 = torch.nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        inputs = x\n",
    "        output = self.layer01(inputs)\n",
    "        return output\n",
    "#\n",
    "linear_model = Linear_model()\n",
    "print(linear_model.state_dict())\n",
    "#\n",
    "lm = linear_model(x)\n",
    "#\n",
    "lm_x  = torch.autograd.grad(outputs=lm, \n",
    "                            inputs=x,\n",
    "                            create_graph=True,\n",
    "                            grad_outputs=torch.ones_like(lm)\n",
    "                            )[0]\n",
    "print(lm_x)\n",
    "#\n",
    "lm_xx = torch.autograd.grad(outputs=lm_x, \n",
    "                            inputs=x,\n",
    "                            grad_outputs=torch.ones_like(lm_x)\n",
    "                            )[0]\n",
    "print(lm_xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env-win",
   "language": "python",
   "name": "torch-env-win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
